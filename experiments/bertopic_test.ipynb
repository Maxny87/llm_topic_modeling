{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:54:46.666070400Z",
     "start_time": "2025-01-09T02:30:11.455084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bertopic\n",
    "import gc\n",
    "import torch\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "\n",
    "def test_model1(dataset, seed, model_file_path, min_topic_size, num_topics, \n",
    "               csv_file_path, bertopic_labels_csv_file_path):\n",
    "    \n",
    "    print(f\"Testing\")\n",
    "\n",
    "    # loading in umap and hbdscan for reproducibility/seed and used default parameters bertopic used\n",
    "    embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=seed)\n",
    "        \n",
    "    # min cluster size is min topic size\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_topic_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "    topic_model = bertopic.BERTopic(nr_topics=num_topics, top_n_words=15, umap_model=umap_model, calculate_probabilities=False,\n",
    "                                    hdbscan_model=hdbscan_model, embedding_model=embedding_model)\n",
    "        \n",
    "    topic_model.fit_transform(dataset)\n",
    "\n",
    "    print(\"Model done fitting\")\n",
    "\n",
    "    # to get keywords per topic, all we need to do is get_topics() on the topic model\n",
    "    keywords_representation = {}\n",
    "    for topic, value in topic_model.get_topics().items():\n",
    "\n",
    "        # will give topic:keywords\n",
    "        keywords = []\n",
    "        for keyword, c_tf_idf in value:\n",
    "            keywords.append(keyword)\n",
    "\n",
    "        keywords_representation[topic] = keywords\n",
    "\n",
    "    bertopic_labels = topic_model.generate_topic_labels(nr_words=3)\n",
    "    \n",
    "\n",
    "    with open(csv_file_path, \"w\", encoding='utf-8') as file:\n",
    "        for topic, keywords in keywords_representation.items():\n",
    "            file.write(f\"Topic {topic}: \\n\")\n",
    "            file.write(f\"Topic Keywords: {keywords} \\n\")\n",
    "            file.write(\"----------------------------------------------------------------------------------------------------------------------------\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    # BERTopic labels\n",
    "    with open(bertopic_labels_csv_file_path, \"w\", encoding='utf-8') as file:\n",
    "        for label in bertopic_labels:\n",
    "            topic = int(label.split('_')[0])\n",
    "            file.write(f\"\\nTopic {topic}: \\n\")\n",
    "            file.write(f\"Topic Keywords: {keywords_representation[topic]}\\n\")\n",
    "            file.write(f\"BERTopic Generated Label: {label} \\n\")\n",
    "            file.write(\"----------------------------------------------------------------------------------------------------------------------------\")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    topic_model.save(model_file_path, serialization=\"safetensors\", save_ctfidf=True,\n",
    "                         save_embedding_model=embedding_model)\n",
    "    \n",
    "    del topic_model\n",
    "    del hdbscan_model\n",
    "    del umap_model\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    \n",
    "print(torch.cuda.is_available())"
   ],
   "id": "1b34d7d1f983a0fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Arxiv Parameter Tests",
   "id": "16952b6163ff8b60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from get_arxiv_abstract_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_arxiv_abstracts_dataset_final.csv')\n",
    "\n",
    "dataset_name = 'arxiv'\n",
    "datasize = len(data)\n",
    "min_topic_size = 300\n",
    "seed = 1\n",
    "nr_topics = 26\n",
    "model_file_path = f'./models/arxiv/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_preprocessed_final'\n",
    "csv_file_path = f'./data/results/arxiv/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_preprocessed_final.csv'\n",
    "bertopic_labels_csv_file_path = f'./data/results/arxiv/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_with_bert_labels_preprocessed_final.csv'\n",
    "\n",
    "test_model1(data, seed, model_file_path, min_topic_size, nr_topics, csv_file_path, bertopic_labels_csv_file_path)"
   ],
   "id": "e0d98d1abada8005",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BBC News Parameter Tests",
   "id": "84ab2ef63df2a58a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from get_bbc_news_data import get_preprocessed_text\n",
    "\n",
    "data = get_preprocessed_text(save_dataset=True)\n",
    "\n",
    "dataset_name = 'bbc_news'\n",
    "datasize = len(data)\n",
    "min_topic_size = 15\n",
    "seed = 1\n",
    "nr_topics = 16\n",
    "model_file_path = f'./models/bbc_news/bertopic_preprocessed_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_final'\n",
    "csv_file_path = f'./data/results/bbc_news/bertopic_preprocessed_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_final.csv'\n",
    "bertopic_labels_csv_file_path = f'./data/results/bbc_news/bertopic_preprocessed_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_with_bert_labels_final.csv'\n",
    "\n",
    "test_model1(data, seed, model_file_path, min_topic_size, nr_topics, csv_file_path, bertopic_labels_csv_file_path)"
   ],
   "id": "ae2ee0a2e037a4a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Amazon Reviews Parameter Tests",
   "id": "ddbdfb46e326d8af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:54:46.666070400Z",
     "start_time": "2024-09-24T15:45:12.973882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_amazon_reviews_data import get_preprocessed_data\n",
    "\n",
    "fraction_to_sample = 0.05\n",
    "seed = 1\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "save_file = f'./data/clean_amazon_reviews_frac={fraction_to_sample}_seed={seed}_chunksize={chunk_size}_retrain'\n",
    "data = get_preprocessed_data('D:\\\\topic_modeling_research\\\\data\\\\Electronics.jsonl', fraction_to_sample, chunk_size, seed, save_dataset=True, save_file_path=save_file)"
   ],
   "id": "6955ff9b3037f0b2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:54:46.666070400Z",
     "start_time": "2024-09-24T17:35:39.146005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_amazon_reviews_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data1 = get_preprocessed_data_from_csv('./data/clean_amazon_reviews_frac=0.05_seed=1_chunksize=1000000_final.csv')\n",
    "data2 = get_preprocessed_data_from_csv('./data/clean_amazon_reviews_frac=0.05_seed=1_chunksize=1000000_retrain.csv')\n",
    "\n",
    "if data1 == data2:\n",
    "    print(\"They are the same\")\n",
    "else:\n",
    "    print(\"They are different\")"
   ],
   "id": "fa2e8cbd6d8c3f24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are the same\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:54:46.681692200Z",
     "start_time": "2024-10-12T05:56:47.231720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_amazon_reviews_data import get_preprocessed_data, get_preprocessed_data_from_csv\n",
    "\n",
    "fraction_to_sample = 0.05\n",
    "seed = 1\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_amazon_reviews_frac=0.05_seed=1_chunksize=1000000_final.csv')\n",
    "    \n",
    "dataset_name = 'amazon_reviews'\n",
    "datasize = len(data)\n",
    "min_topic_size = 400\n",
    "nr_topics = 21\n",
    "model_file_path = f'./models/amazon_reviews/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_preprocessed_retrain_2'\n",
    "csv_file_path = f'./data/results/amazon_reviews/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_preprocessed_retrain_2.csv'\n",
    "bertopic_labels_csv_file_path = f'./data/results/amazon_reviews/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_with_bert_labels_preprocessed_retrain_2.csv'\n",
    "    \n",
    "test_model1(data, seed, model_file_path, min_topic_size, nr_topics, csv_file_path, bertopic_labels_csv_file_path)"
   ],
   "id": "de7707d3f349c904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Model done fitting\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Newsgroup20 Parameter Tests",
   "id": "eaa11ae4866d81ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:54:46.681692200Z",
     "start_time": "2024-08-13T14:27:19.315851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_news_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_newsgroup20_final.csv')\n",
    "\n",
    "dataset_name = 'newsgroup20'\n",
    "datasize = len(data)\n",
    "min_topic_size = 25\n",
    "seed = 1\n",
    "nr_topics = 26\n",
    "model_file_path = f'./models/newsgroup20/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_preprocessed_final'\n",
    "csv_file_path = f'./data/results/newsgroup20/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_preprocessed_final.csv'\n",
    "bertopic_labels_csv_file_path = f'./data/results/newsgroup20/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_with_bert_labels_preprocessed_final.csv'\n",
    "\n",
    "test_model1(data, seed, model_file_path, min_topic_size, nr_topics, csv_file_path, bertopic_labels_csv_file_path)"
   ],
   "id": "9ab00ffb5e4482b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Model done fitting\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WorldCup Tweets Parameter Tests",
   "id": "de774298a7613d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:54:46.681692200Z",
     "start_time": "2024-08-14T14:59:03.496600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_worldcup_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_worldcup_final.csv')\n",
    "\n",
    "dataset_name = 'worldcup_tweets'\n",
    "datasize = len(data)\n",
    "min_topic_size = 250\n",
    "seed = 1\n",
    "nr_topics = 16\n",
    "model_file_path = f'./models/worlcup/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_preprocessed_final'\n",
    "csv_file_path = f'./data/results/worlcup/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_preprocessed_final.csv'\n",
    "bertopic_labels_csv_file_path = f'./data/results/worlcup/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_with_bert_labels_preprocessed_final.csv'\n",
    "\n",
    "test_model1(data, seed, model_file_path, min_topic_size, nr_topics, csv_file_path, bertopic_labels_csv_file_path)"
   ],
   "id": "a9f3b8fe84bb384a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Model done fitting\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T16:54:46.681692200Z",
     "start_time": "2024-08-14T16:52:17.533464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_worldcup_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_worldcup_final.csv')\n",
    "\n",
    "dataset_name = 'worldcup_tweets'\n",
    "datasize = len(data)\n",
    "min_topic_size = 350\n",
    "seed = 1\n",
    "nr_topics = 26\n",
    "model_file_path = f'./models/worlcup/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_preprocessed_final'\n",
    "csv_file_path = f'./data/results/worlcup/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_preprocessed_final.csv'\n",
    "bertopic_labels_csv_file_path = f'./data/results/worlcup/bertopic_{dataset_name}_{datasize}_{seed}_{min_topic_size}_{nr_topics}_topic_labels_with_bert_labels_preprocessed_final.csv'\n",
    "\n",
    "test_model1(data, seed, model_file_path, min_topic_size, nr_topics, csv_file_path, bertopic_labels_csv_file_path)"
   ],
   "id": "6dff59266838adcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Model done fitting\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizations For BERTopic",
   "id": "690848f0d3a890ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T19:52:05.386445Z",
     "start_time": "2025-01-21T19:51:51.141324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bertopic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import importlib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "\n",
    "amazon_model_path = './models/amazon_reviews/bertopic_amazon_reviews_2043299_1_400_21_preprocessed_retrain'\n",
    "arxiv_model_path = './models/arxiv/bertopic_arxiv_2521247_1_300_26_preprocessed_final'\n",
    "bbc_news_model_path = './models/bbc_news/bertopic_preprocessed_bbc_news_2225_1_15_16_final'\n",
    "newsgroup20_model_path = './models/newsgroup20/bertopic_newsgroup20_18811_1_25_26_preprocessed_final'\n",
    "\n",
    "dataset_dict = {\n",
    "    'amazon_reviews_dataset': {'filename': 'get_amazon_reviews_data',\n",
    "                               'model_path': amazon_model_path, \n",
    "                               'csv_file_path': './data/clean_amazon_reviews_frac=0.05_seed=1_chunksize=1000000_final.csv'},\n",
    "    'bbc_news_dataset': {'filename': 'get_bbc_news_data',\n",
    "                         'model_path': bbc_news_model_path,\n",
    "                         'csv_file_path': './data/clean_bbc_news_dataset_final.csv'},\n",
    "    'newsgroup20_dataset': {'filename': 'get_news_data',\n",
    "                            'model_path': newsgroup20_model_path,\n",
    "                            'csv_file_path': './data/clean_newsgroup20_final.csv'},\n",
    "    'arxiv_dataset': {'filename': 'get_arxiv_abstract_data',\n",
    "                      'model_path': arxiv_model_path,\n",
    "                      'csv_file_path': './data/clean_arxiv_abstracts_dataset_final.csv'},\n",
    "}\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sentence_model = SentenceTransformer(embedding_model)\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=1)"
   ],
   "id": "a03e310af80f5c7b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T19:52:13.196616Z",
     "start_time": "2025-01-21T19:52:05.395848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_news_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_newsgroup20_final.csv')\n",
    "\n",
    "topic_model = bertopic.BERTopic.load(newsgroup20_model_path)\n",
    "\n",
    "# visualize topics\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"Newsgroup20: Topic Distribution\",\n",
    "#     #     font=dict(size=24),  # Larger font for title\n",
    "#     #     x=0.5,  # Center the title\n",
    "#     #     xanchor=\"center\"\n",
    "#     # ),\n",
    "#     width=900,  # Adjusting width for better aspect ratio\n",
    "#     height=700,  # Adjusting height to match\n",
    "#     # xaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing x-axis range\n",
    "#     # yaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing y-axis range\n",
    "#     font=dict(size=18)  # Smaller general font size for journal readability\n",
    "# )\n",
    "# fig.write_image(\"./plots/newsgroup20/newsgroup20_visualize_topics_bigger.png\")\n",
    "\n",
    "# visualize wordweights\n",
    "fig = topic_model.visualize_barchart(top_n_topics=25)\n",
    "fig.update_layout(\n",
    "    # title=dict(\n",
    "    #     text=\"Amazon Electronics Reviews: Top Words Per Topic\",\n",
    "    #     font=dict(size=24),  # Larger font for title\n",
    "    #     x=0.5,  # Center the title\n",
    "    #     xanchor=\"center\"\n",
    "    # ),\n",
    "    width=1500,  # Slightly narrower to fit single-column width\n",
    "    height=1000,  # Adjusted for balance\n",
    "    font=dict(size=22) # Consistent font size for readability\n",
    ")\n",
    "\n",
    "# Iterate over all x-axes in the subplots\n",
    "for i in range(1, 26):  # Assuming there are 25 topics (1-based index for subplots)\n",
    "    fig['layout'][f'xaxis{i}']['tickfont']['size'] = 14\n",
    "    fig['layout'][f'xaxis{i}']['title']['font']['size'] = 16\n",
    "\n",
    "# Adjust annotation font sizes\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font']['size'] = 12  # Slightly larger for journal readability\n",
    "\n",
    "\n",
    "fig.write_image(\"./plots/newsgroup20/newsgroup20_wordweights_bigger.png\")\n",
    "\n",
    "# visualize documents\n",
    "# embeddings = sentence_model.encode(data)\n",
    "# reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "#\n",
    "# fig = topic_model.visualize_documents(\n",
    "#     data,\n",
    "#     reduced_embeddings=reduced_embeddings,\n",
    "#     hide_document_hover=True,\n",
    "#     hide_annotations=True\n",
    "# )\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"Newsgroup20: Document Clustering\",\n",
    "#     # ),\n",
    "#     width=900,\n",
    "#     height=700,\n",
    "#     font=dict(size=20),\n",
    "#     legend=dict(font=dict(size=16)),\n",
    "# )\n",
    "# fig.write_image(\"./plots/newsgroup20/newsgroup20_visualize_documents_bigger.png\")\n",
    "#\n",
    "# del data\n",
    "# del topic_model\n",
    "# del embeddings\n",
    "# del reduced_embeddings"
   ],
   "id": "da53315807d34efe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T19:52:31.998217Z",
     "start_time": "2025-01-21T19:52:13.482311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_arxiv_abstract_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_arxiv_abstracts_dataset_final.csv')\n",
    "\n",
    "topic_model = bertopic.BERTopic.load(arxiv_model_path)\n",
    "\n",
    "# visualize topics\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"Arxiv Abstracts: Topic Distribution\",\n",
    "#     #     font=dict(size=24),  # Larger font for title\n",
    "#     #     x=0.5,  # Center the title\n",
    "#     #     xanchor=\"center\"\n",
    "#     # ),\n",
    "#     width=900,  # Adjusting width for better aspect ratio\n",
    "#     height=700,  # Adjusting height to match\n",
    "#     # xaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing x-axis range\n",
    "#     # yaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing y-axis range\n",
    "#     font=dict(size=18) # Smaller general font size for journal readability\n",
    "# )\n",
    "# fig.write_image(\"./plots/arxiv/arxiv_visualize_topics_bigger.png\")\n",
    "\n",
    "# visualize wordweights\n",
    "fig = topic_model.visualize_barchart(top_n_topics=25)\n",
    "fig.update_layout(\n",
    "    # title=dict(\n",
    "    #     text=\"Amazon Electronics Reviews: Top Words Per Topic\",\n",
    "    #     font=dict(size=24),  # Larger font for title\n",
    "    #     x=0.5,  # Center the title\n",
    "    #     xanchor=\"center\"\n",
    "    # ),\n",
    "    width=1500,  # Slightly narrower to fit single-column width\n",
    "    height=1000,  # Adjusted for balance\n",
    "    font=dict(size=22) # Consistent font size for readability\n",
    ")\n",
    "\n",
    "# Iterate over all x-axes in the subplots\n",
    "for i in range(1, 26):  # Assuming there are 25 topics (1-based index for subplots)\n",
    "    fig['layout'][f'xaxis{i}']['tickfont']['size'] = 14\n",
    "    fig['layout'][f'xaxis{i}']['title']['font']['size'] = 16\n",
    "\n",
    "# Adjust annotation font sizes\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font']['size'] = 12  # Slightly larger for journal readability\n",
    "    \n",
    "fig.write_image(\"./plots/arxiv/arxiv_wordweights_bigger.png\")\n",
    "\n",
    "# visualize documents\n",
    "# embeddings = sentence_model.encode(data)\n",
    "# reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "#\n",
    "# fig = topic_model.visualize_documents(\n",
    "#     data,\n",
    "#     reduced_embeddings=reduced_embeddings,\n",
    "#     hide_document_hover=True,\n",
    "#     hide_annotations=True\n",
    "# )\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"Arxiv Abstracts: Document Clustering\",\n",
    "#     # ),\n",
    "#     width=900,\n",
    "#     height=700,\n",
    "#     font=dict(size=20),\n",
    "#     legend=dict(font=dict(size=16)),\n",
    "# )\n",
    "# fig.write_image(\"./plots/arxiv/arxiv_visualize_documents_bigger.png\")\n",
    "#\n",
    "# del data\n",
    "# del topic_model\n",
    "# del embeddings\n",
    "# del reduced_embeddings"
   ],
   "id": "cdbd5a485efa3b12",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T19:52:33.660959Z",
     "start_time": "2025-01-21T19:52:32.038538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_bbc_news_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_bbc_news_dataset_final.csv')\n",
    "\n",
    "topic_model = bertopic.BERTopic.load(bbc_news_model_path)\n",
    "\n",
    "# visualize topics\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"BBC News Articles: Topic Distribution\",\n",
    "#     #     font=dict(size=24),  # Larger font for title\n",
    "#     #     x=0.5,  # Center the title\n",
    "#     #     xanchor=\"center\"\n",
    "#     # ),\n",
    "#     width=900,  # Adjusting width for better aspect ratio\n",
    "#     height=700,  # Adjusting height to match\n",
    "#     # xaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing x-axis range\n",
    "#     # yaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing y-axis range\n",
    "#     font=dict(size=18)  # Smaller general font size for journal readability\n",
    "# )\n",
    "# fig.write_image(\"./plots/bbc_news/bbc_news_visualize_topics_bigger.png\")\n",
    "\n",
    "\n",
    "# visualize wordweights\n",
    "fig = topic_model.visualize_barchart(top_n_topics=15)\n",
    "fig.update_layout(\n",
    "    # title=dict(\n",
    "    #     text=\"Amazon Electronics Reviews: Top Words Per Topic\",\n",
    "    #     font=dict(size=24),  # Larger font for title\n",
    "    #     x=0.5,  # Center the title\n",
    "    #     xanchor=\"center\"\n",
    "    # ),\n",
    "    width=1500,  # Slightly narrower to fit single-column width\n",
    "    height=1000,  # Adjusted for balance\n",
    "    font=dict(size=22) # Consistent font size for readability\n",
    ")\n",
    "\n",
    "# Iterate over all x-axes in the subplots\n",
    "for i in range(1, 16):  # Assuming there are 25 topics (1-based index for subplots)\n",
    "    fig['layout'][f'xaxis{i}']['tickfont']['size'] = 14\n",
    "    fig['layout'][f'xaxis{i}']['title']['font']['size'] = 16\n",
    "\n",
    "# Adjust annotation font sizes\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font']['size'] = 12  # Slightly larger for journal readability\n",
    "\n",
    "fig.write_image(\"./plots/bbc_news/bbc_news_wordweights_bigger.png\")\n",
    "\n",
    "# visualize documents\n",
    "# embeddings = sentence_model.encode(data)\n",
    "# reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "#\n",
    "# fig = topic_model.visualize_documents(\n",
    "#     data,\n",
    "#     reduced_embeddings=reduced_embeddings,\n",
    "#     hide_document_hover=True,\n",
    "#     hide_annotations=True\n",
    "# )\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"BBC News Articles: Document Clustering\",\n",
    "#     # ),\n",
    "#     width=900,\n",
    "#     height=700,\n",
    "#     font=dict(size=20),\n",
    "#     legend=dict(font=dict(size=16)),\n",
    "# )\n",
    "# fig.write_image(\"./plots/bbc_news/bbc_news_visualize_documents_bigger.png\")\n",
    "#\n",
    "# del data\n",
    "# del topic_model\n",
    "# del embeddings\n",
    "# del reduced_embeddings"
   ],
   "id": "f84b1ad90cc3827a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T19:52:40.361906Z",
     "start_time": "2025-01-21T19:52:33.695344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from get_amazon_reviews_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('./data/clean_amazon_reviews_frac=0.05_seed=1_chunksize=1000000_final.csv')\n",
    "\n",
    "topic_model = bertopic.BERTopic.load(amazon_model_path)\n",
    "\n",
    "# visualize topics\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"Amazon Electronics Reviews: Topic Distribution\",\n",
    "#     #     font=dict(size=24),  # Larger font for title\n",
    "#     #     x=0.5,  # Center the title\n",
    "#     #     xanchor=\"center\"\n",
    "#     # ),\n",
    "#     width=900,  # Adjusting width for better aspect ratio\n",
    "#     height=700,  # Adjusting height to match\n",
    "#     # xaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing x-axis range\n",
    "#     # yaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing y-axis range\n",
    "#     font=dict(size=18)  # Smaller general font size for journal readability\n",
    "# )\n",
    "# fig.write_image(\"./plots/amazon_reviews/amazon_visualize_topics_bigger.png\")\n",
    "\n",
    "# visualize wordweights\n",
    "fig = topic_model.visualize_barchart(top_n_topics=20)\n",
    "fig.update_layout(\n",
    "    # title=dict(\n",
    "    #     text=\"Amazon Electronics Reviews: Top Words Per Topic\",\n",
    "    #     font=dict(size=24),  # Larger font for title\n",
    "    #     x=0.5,  # Center the title\n",
    "    #     xanchor=\"center\"\n",
    "    # ),\n",
    "    width=1500,  # Slightly narrower to fit single-column width\n",
    "    height=1000,  # Adjusted for balance\n",
    "    font=dict(size=22) # Consistent font size for readability\n",
    ")\n",
    "\n",
    "# Iterate over all x-axes in the subplots\n",
    "for i in range(1, 21):  # Assuming there are 25 topics (1-based index for subplots)\n",
    "    fig['layout'][f'xaxis{i}']['tickfont']['size'] = 14\n",
    "    fig['layout'][f'xaxis{i}']['title']['font']['size'] = 16\n",
    "\n",
    "# Adjust annotation font sizes\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font']['size'] = 12  # Slightly larger for journal readability\n",
    "    \n",
    "fig.write_image(\"./plots/amazon_reviews/amazon_wordweights_bigger.png\")\n",
    "\n",
    "# visualize documents\n",
    "# embeddings = sentence_model.encode(data)\n",
    "# reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "#\n",
    "# fig = topic_model.visualize_documents(\n",
    "#     data,\n",
    "#     reduced_embeddings=reduced_embeddings,\n",
    "#     hide_document_hover=True,\n",
    "#     hide_annotations=True\n",
    "# )\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"Amazon Electronic Reviews: Document Clustering\",\n",
    "#     # ),\n",
    "#     width=900,\n",
    "#     height=700,\n",
    "#     font=dict(size=20),\n",
    "#     legend=dict(font=dict(size=16)),\n",
    "# )\n",
    "# fig.write_image(\"./plots/amazon_reviews/amazon_visualize_documents_bigger.png\")\n",
    "#\n",
    "# del data\n",
    "# del topic_model\n",
    "# del embeddings\n",
    "# del reduced_embeddings"
   ],
   "id": "68b49d5c83a89223",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T19:52:44.336858Z",
     "start_time": "2025-01-21T19:52:40.395496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "world_cup_model_path = \"./models/worlcup/bertopic_worldcup_tweets_2407246_1_350_26_preprocessed_final\"\n",
    "\n",
    "from get_worldcup_data import get_preprocessed_data_from_csv\n",
    "\n",
    "data = get_preprocessed_data_from_csv('D:\\\\TopicMoldeing_Redo\\\\data\\\\clean_worldcup_final.csv')\n",
    "\n",
    "topic_model = bertopic.BERTopic.load(world_cup_model_path)\n",
    "\n",
    "# visualize topics\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"World Cup Tweets: Topic Distribution\",\n",
    "#     #     font=dict(size=24),  # Larger font for title\n",
    "#     #     x=0.5,  # Center the title\n",
    "#     #     xanchor=\"center\"\n",
    "#     # ),\n",
    "#     width=900,  # Adjusting width for better aspect ratio\n",
    "#     height=700,  # Adjusting height to match\n",
    "#     # xaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing x-axis range\n",
    "#     # yaxis=dict(range=[-20, 20], title_font=dict(size=14)),  # Narrowing y-axis range\n",
    "#     font=dict(size=18) # Smaller general font size for journal readability\n",
    "# )\n",
    "# fig.write_image(\"./plots/worldcup/worldcup_visualize_topics_bigger.png\")\n",
    "\n",
    "# visualize wordweights\n",
    "topics = [0, 1, 2, 3, 4, 11, 12, 15, 16, 18, 19, 22]\n",
    "fig = topic_model.visualize_barchart(topics=topics)\n",
    "fig.update_layout(\n",
    "    # title=dict(\n",
    "    #     text=\"Amazon Electronics Reviews: Top Words Per Topic\",\n",
    "    #     font=dict(size=24),  # Larger font for title\n",
    "    #     x=0.5,  # Center the title\n",
    "    #     xanchor=\"center\"\n",
    "    # ),\n",
    "    width=1500,  # Slightly narrower to fit single-column width\n",
    "    height=1000,  # Adjusted for balance\n",
    "    font=dict(size=22) # Consistent font size for readability\n",
    ")\n",
    "\n",
    "# Iterate over all x-axes in the subplots\n",
    "for i in range(1, 13):  # Assuming there are 25 topics (1-based index for subplots)\n",
    "    fig['layout'][f'xaxis{i}']['tickfont']['size'] = 14\n",
    "    fig['layout'][f'xaxis{i}']['title']['font']['size'] = 16\n",
    "\n",
    "# Adjust annotation font sizes\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font']['size'] = 12  # Slightly larger for journal readability\n",
    "    \n",
    "for i, topic_num in enumerate(topics):\n",
    "    custom_title = f\"Topic {i}\"\n",
    "    fig['layout']['annotations'][i]['text'] = custom_title\n",
    "\n",
    "fig.write_image(\"./plots/worldcup/worldcup_wordweights_bigger.png\")\n",
    "\n",
    "# visualize documents\n",
    "# embeddings = sentence_model.encode(data)\n",
    "# reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "#\n",
    "# fig = topic_model.visualize_documents(\n",
    "#     data,\n",
    "#     reduced_embeddings=reduced_embeddings,\n",
    "#     hide_document_hover=True,\n",
    "#     hide_annotations=True\n",
    "# )\n",
    "# fig.update_layout(\n",
    "#     # title=dict(\n",
    "#     #     text=\"World Cup Tweets: Document Clustering\",\n",
    "#     # ),\n",
    "#     width=900,\n",
    "#     height=700,\n",
    "#     font=dict(size=20),\n",
    "#     legend=dict(font=dict(size=16)),\n",
    "# )\n",
    "# fig.write_image(\"./plots/worldcup/worldcup_visualize_documents_bigger.png\")\n",
    "#\n",
    "# del data\n",
    "# del topic_model\n",
    "# del embeddings\n",
    "# del reduced_embeddings"
   ],
   "id": "24a165b4ff46df3d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:29:07.539504Z",
     "start_time": "2025-01-21T01:29:07.536579Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "544e127abf8fd987",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "27630cc63f9c0cd7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
